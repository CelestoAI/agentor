---
description: Defines the vector-based memory architecture for storing, retrieving and searching conversation history and agent interactions.
---


# memory-system-architecture

The memory system implements a vector-based conversation storage architecture using the following key components:

## Core Memory Components (Importance Score: 75)
Path: src/agentor/memory/api.py

1. Vector Storage Engine
- LanceDB-based conversation history storage
- Custom embedding schema for dialog chunks
- Versioned conversation context handling
- Semantic retrieval capabilities

2. Embedding Management
- Sentence transformer model: google/embeddinggemma-300M
- Conversation chunk vectorization
- Custom Words model for vector operations

## Memory Operations (Importance Score: 70)

1. Conversation Management
- Semantic search across conversation history
- Complete context retrieval functions
- Incremental memory updates
- Contextual relevance scoring

2. Storage Patterns
- Vector-based conversation indexing
- Dialog chunk segmentation
- Context window management
- Conversation version control

## Memory Integration Points (Importance Score: 80)

1. Agent Memory Interface
- Conversation context injection
- Memory-aware agent initialization
- Historical context retrieval
- Semantic search capabilities

2. Memory Persistence
- Embedding schema versioning
- Conversation state management
- Context retention policies
- Memory pruning rules

The memory architecture prioritizes semantic understanding and contextual relevance while maintaining conversation history integrity through vector-based storage and retrieval mechanisms.

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga memory-system-architecture" along with specifying exactly what information was used from this file in a human-friendly way, instead of using kebab-case use normal sentence case.